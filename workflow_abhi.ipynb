{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy patsy plotly plotly_express nbformat\n",
    "\n",
    "#Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy as pt\n",
    "import plotly.express as px\n",
    "import nbformat\n",
    "import plotly as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv(\"amazon-purchases.csv\")\n",
    "survey_data = pd.read_csv(\"survey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a prime purchase column to the data\n",
    "\n",
    "#Create a list with prime days inside it\n",
    "prime_days = [\"2022-07-12\", \"2022-07-13\", \"2021-06-21\", \"2021-06-22\", \"2020-10-13\", \"2020-10-14\", \"2019-07-15\", \"2019-07-16\", \"2018-07-17\", \"2018-07-18\",]\n",
    "\n",
    "prime_purchase = []\n",
    "for i in data[\"Order Date\"]:\n",
    "    if i in prime_days:\n",
    "        prime_purchase.append(1)\n",
    "    else:\n",
    "        prime_purchase.append(0)\n",
    "data[\"Prime Purchase\"] = prime_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use prime purchase data to create a prime day customer list\n",
    "prime_day_customer = []\n",
    "for i in range(len(data)):\n",
    "    if data[\"Prime Purchase\"][i] == 1:\n",
    "        prime_day_customer.append(data[\"Survey ResponseID\"][i])\n",
    "prime_day_customer = set(prime_day_customer)\n",
    "\n",
    "prime_customer = []\n",
    "for i in data[\"Survey ResponseID\"]:\n",
    "    if i in prime_day_customer:\n",
    "        prime_customer.append(1)\n",
    "    else:\n",
    "        prime_customer.append(0)\n",
    "\n",
    "data[\"Prime Customer\"] = prime_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the purchase data with customer surveys on 'Survey ResponseID'\n",
    "merged_data = pd.merge(data, survey_data, on='Survey ResponseID', how='inner')\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(merged_data.shape, data.shape, survey_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = merged_data[(merged_data['Q-demos-gender'] == 'Female') & (merged_data['Q-demos-age'] != '65 and older') & (merged_data['Prime Customer'] == 1) & (merged_data['Q-amazon-use-howmany']=='1 (just me!)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data['Revenue'] = subset_data['Quantity'] * subset_data['Purchase Price Per Unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_responses_by_gender = merged_data.groupby('Q-demos-gender')['Survey ResponseID'].nunique()\n",
    "print(unique_responses_by_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data['Order Date'].max()\n",
    "record = subset_data[merged_data['Order Date'] == subset_data['Order Date'].max()]\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' to datetime\n",
    "subset_data.loc[:, 'Order Date'] = pd.to_datetime(subset_data['Order Date'])\n",
    "\n",
    "# Group by 'Order Date' and sum the 'Purchase Price Per Unit'\n",
    "transaction_totals = subset_data.groupby(['Order Date', 'Q-demos-gender'])['Purchase Price Per Unit'].sum().reset_index()\n",
    "\n",
    "# Plot the time series\n",
    "px.line(transaction_totals, x='Order Date', y='Purchase Price Per Unit', title='Transaction Totals Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' to datetime\n",
    "subset_data['Order Date'] = pd.to_datetime(subset_data['Order Date'])\n",
    "\n",
    "# Extract year, month, day, and day of the week\n",
    "subset_data['Year'] = subset_data['Order Date'].dt.year\n",
    "subset_data['Month'] = subset_data['Order Date'].dt.month\n",
    "subset_data['Day'] = subset_data['Order Date'].dt.day\n",
    "subset_data['Day of Week'] = subset_data['Order Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means for purchase quantity and purchase totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by survey response id and count the number of prime purchases and sum the price per unit\n",
    "prime_users = subset_data.groupby(\"Survey ResponseID\").agg({\"Prime Purchase\":\"sum\", \"Purchase Price Per Unit\":\"sum\"})\n",
    "#Left join prime purchases with survey data\n",
    "print(prime_users.shape)\n",
    "#Rename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn matplotlib numpy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_data = list(zip(prime_users['Prime Purchase'], prime_users['Purchase Price Per Unit']))\n",
    "inertias = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(k_data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(k_data)\n",
    "\n",
    "plt.scatter(prime_users['Prime Purchase'], prime_users['Purchase Price Per Unit'], c=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Display the labels assigned to each data point\n",
    "print(\"Cluster Labels:\")\n",
    "print(kmeans.labels_)\n",
    "\n",
    "prime_users['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'cluster' column is present in prime_users\n",
    "if 'cluster' not in prime_users.columns:\n",
    "\tprime_users['cluster'] = kmeans.labels_\n",
    "\n",
    "# Merge the subset_data with prime_users to map the cluster labels\n",
    "subset_data_with_clusters = pd.merge(subset_data, prime_users[['cluster']], on='Survey ResponseID', how='left')\n",
    "\n",
    "# Display the first few rows of the resulting dataframe\n",
    "subset_data_with_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and category, then sum the purchase totals\n",
    "cluster_category_totals = subset_data_with_clusters.groupby(['cluster', 'Category'])['Purchase Price Per Unit'].sum().reset_index()\n",
    "\n",
    "# Sort the totals within each cluster and select the top 5 categories\n",
    "top_categories_per_cluster = cluster_category_totals.sort_values(['cluster', 'Purchase Price Per Unit'], ascending=[True, False]).groupby('cluster').head(10)\n",
    "\n",
    "# Display the result\n",
    "top_categories_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prices = subset_data.groupby(['Category','Prime Purchase'])['Revenue'].mean()\n",
    "\n",
    "# Calculate the difference in mean between prime purchase 0 and 1 for each category\n",
    "average_prices_diff = average_prices.unstack().diff(axis=1).iloc[:, -1].reset_index()\n",
    "average_prices_diff.columns = ['Category', 'Difference']\n",
    "\n",
    "# Display the top 40 values\n",
    "top_40_differences = average_prices_diff.nlargest(40, 'Difference')\n",
    "top_40_differences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Category and Prime Purchase, then sum the Quantity\n",
    "quantity_summed = subset_data.groupby(['Category', 'Prime Purchase'])['Quantity'].sum().unstack().reset_index()\n",
    "\n",
    "# Calculate the difference in quantity between prime purchase 0 and 1\n",
    "quantity_summed['Quantity Difference'] = quantity_summed[1] - quantity_summed[0]\n",
    "\n",
    "# Calculate the total quantity for each category\n",
    "quantity_summed['Quantity Total'] = quantity_summed[0] + quantity_summed[1]\n",
    "\n",
    "# Merge with top_40_differences to display the results together\n",
    "result = pd.merge(top_40_differences, quantity_summed[['Category', 'Quantity Difference', 'Quantity Total']], on='Category', how='left')\n",
    "\n",
    "# Sort by 'Quantity Difference' and select the top 40\n",
    "result = result.sort_values(by='Quantity Difference', ascending=False).head(40)\n",
    "\n",
    "# Display the result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prices.to_csv('average_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_with_clusters.to_csv('subset_data_with_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and Date and aggregate the quantity and purchase prices\n",
    "aggregated_data = subset_data.groupby(['Order Date']).agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Revenue': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "#Add a column for the year, month, day, and day of the week\n",
    "aggregated_data['Year'] = aggregated_data['Order Date'].dt.year\n",
    "aggregated_data['Month'] = aggregated_data['Order Date'].dt.month\n",
    "aggregated_data['Day'] = aggregated_data['Order Date'].dt.day\n",
    "aggregated_data['Day of Week'] = aggregated_data['Order Date'].dt.dayofweek\n",
    "\n",
    "# Display the first few rows of the aggregated data\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify and Prepare the Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential Smoothing\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "import plotly_express as px\n",
    "\n",
    "px.line(aggregated_data, x='Order Date', y='Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = aggregated_data['Revenue']\n",
    "revenue.index = aggregated_data['Order Date']\n",
    "revenue.index.freq = revenue.index.inferred_freq\n",
    "\n",
    "alpha020 = SimpleExpSmoothing(revenue).fit(\n",
    "                                        smoothing_level=0.2,\n",
    "                                        optimized=False)\n",
    "\n",
    "alpha050 = SimpleExpSmoothing(revenue).fit(\n",
    "                                        smoothing_level=0.5,\n",
    "                                        optimized=False)\n",
    "\n",
    "alpha080 = SimpleExpSmoothing(revenue).fit(\n",
    "                                        smoothing_level=0.8,\n",
    "                                        optimized=False)\n",
    "\n",
    "forecast020 = alpha020.forecast(3)\n",
    "forecast050 = alpha050.forecast(3)\n",
    "forecast080 = alpha080.forecast(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plotting our data\n",
    "\n",
    "smoothData = pd.DataFrame([revenue.values, alpha020.fittedvalues.values,  alpha050.fittedvalues.values,  alpha080.fittedvalues.values]).T\n",
    "smoothData.columns = ['Truth', 'alpha=0.2', 'alpha=0.5', 'alpha=0.8']\n",
    "smoothData.index = revenue.index\n",
    "\n",
    "fig = px.line(smoothData, y = ['Truth', 'alpha=0.2', 'alpha=0.5', 'alpha=0.8'],\n",
    "        x = smoothData.index,\n",
    "        color_discrete_map={\"Truth\": 'blue',\n",
    "                           'alpha=0.2': 'red',\n",
    "                            'alpha=0.5':'green',\n",
    "                            'alpha=0.8':'purple'}\n",
    "       )\n",
    "\n",
    "# Incorporating the Forecasts\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast020.index, y = forecast020.values, name='Forecast alpha=0.2', line={'color':'red'}))\n",
    "fig.add_trace(go.Scatter(x=forecast050.index, y = forecast050.values, name='Forecast alpha=0.5', line={'color':'green'}))\n",
    "fig.add_trace(go.Scatter(x=forecast080.index, y = forecast080.values, name='Forecast alpha=0.8', line={'color':'purple'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GAMs\n",
    "!pip install pygam\n",
    "\n",
    "from pygam import LinearGAM, s, f\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.stattools as st\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.rename(columns={'Day of Week': 'Weekday', 'Purchase Price Per Unit': 'Revenue'}, inplace=True)\n",
    "aggregated_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = aggregated_data[[\"Year\", \"Month\", \"Day\"]]\n",
    "y = aggregated_data[\"Revenue\"]\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GAM model with smooth terms for each feature\n",
    "gam = LinearGAM(s(0) + s(1) + s(2)).fit(x_train, y_train)\n",
    "\n",
    "# Display model summary\n",
    "print(gam.summary())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gam.predict(x_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Plot smooth terms\n",
    "for i, term in enumerate([\"Year\", \"Month\", \"Day\"]):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Smooth Term for {term}\")\n",
    "    gam.partial_dependence(term=i, X=x_test, width=0.95)\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Perform grid search for optimal smoothness\n",
    "gam.gridsearch(x_train, y_train, lam=np.logspace(-3, 3, 11))\n",
    "\n",
    "# Display the updated summary\n",
    "print(gam.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "\n",
    "# Assuming gam is your trained LinearGAM model\n",
    "gam = LinearGAM().fit(x, y)\n",
    "\n",
    "# Plot partial dependence\n",
    "gam.summary()  # Summary should list all feature splines\n",
    "\n",
    "# Plot each term (feature)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(15, 15))  # Adjust nrows to the number of features\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n",
    "    ax.set_title(f\"Partial dependence for term {i}\")\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.show()\n",
    "\n",
    "XX = gam.generate_X_grid(term=0)\n",
    "print(XX)\n",
    "pd = gam.partial_dependence(term=0, X=XX)\n",
    "plt.plot(XX[:, 0], pd)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "\n",
    "# Fit the model without the third feature\n",
    "gam = LinearGAM().fit(x.iloc[:, :2], y)\n",
    "gam.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(y, gam.deviance_residuals(x.iloc[:, :2], y))\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Select only the first two columns of x_test\n",
    "x_test_selected = x_test.iloc[:, :2]\n",
    "\n",
    "y_pred = gam.predict(x_test_selected)\n",
    "print(\"R^2:\", r2_score(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i, term in enumerate(gam.terms):\n",
    "    if term.isintercept:\n",
    "        continue\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
    "    plt.title(f\"Partial Dependence of Feature {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "import numpy as np\n",
    "\n",
    "# Perform grid search for optimal smoothness\n",
    "gam = LinearGAM().gridsearch(x.values, y.values, lam=np.logspace(-3, 3, 11))\n",
    "gam.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate dates for the next 3 months\n",
    "future_dates = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\", freq=\"D\")\n",
    "\n",
    "# Generate the same features as used in the training data\n",
    "future_features = pd.DataFrame({\n",
    "    \"Year\": future_dates.year,\n",
    "    \"Month\": future_dates.month,\n",
    "    \"Day\": future_dates.day\n",
    "})\n",
    "\n",
    "# Assume `gam` is your trained LinearGAM model\n",
    "future_predictions = gam.predict(future_features)\n",
    "\n",
    "# Get confidence intervals\n",
    "intervals = gam.prediction_intervals(future_features, width=0.95)\n",
    "lower = intervals[:, 0]  # Lower bounds\n",
    "upper = intervals[:, 1]  # Upper bounds\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(future_dates, future_predictions, label=\"Predicted Revenue\", color=\"blue\")\n",
    "plt.fill_between(\n",
    "    future_dates, lower, upper, color=\"gray\", alpha=0.2, label=\"Confidence Interval\"\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.title(\"Predicted Daily Revenue for the Next 3 Months\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions to a CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"date\": future_dates,\n",
    "    \"predicted_revenue\": future_predictions,\n",
    "    \"lower_bound\": lower,\n",
    "    \"upper_bound\": upper\n",
    "})\n",
    "predictions_df.to_csv(\"revenue_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "# Create the model and fit it\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree \"clf\"\n",
    "\n",
    "print(\"\\n\\nIn-sample accuracy: %s%%\\n\\n\" \n",
    " % str(round(100*accuracy_score(y, clf.predict(x)), 2)))\n",
    "print(\"\\n\\nOut-of-sample accuracy: %s%%\\n\\n\"\n",
    "%str(round(100*accuracy_score(yt, clf.predict(xt)), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
